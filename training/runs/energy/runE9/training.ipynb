{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_170 (Conv2D)          (None, 5, 512, 32)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_171 (Conv2D)          (None, 5, 512, 32)        5152      \n",
      "_________________________________________________________________\n",
      "conv2d_172 (Conv2D)          (None, 5, 512, 32)        5152      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_52 (Averag (None, 5, 256, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_173 (Conv2D)          (None, 5, 256, 64)        10304     \n",
      "_________________________________________________________________\n",
      "conv2d_174 (Conv2D)          (None, 5, 256, 64)        20544     \n",
      "_________________________________________________________________\n",
      "conv2d_175 (Conv2D)          (None, 5, 256, 64)        20544     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_53 (Averag (None, 5, 128, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_176 (Conv2D)          (None, 5, 128, 128)       41088     \n",
      "_________________________________________________________________\n",
      "conv2d_177 (Conv2D)          (None, 5, 128, 128)       82048     \n",
      "_________________________________________________________________\n",
      "conv2d_178 (Conv2D)          (None, 5, 128, 128)       82048     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_54 (Averag (None, 5, 64, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_179 (Conv2D)          (None, 5, 64, 256)        164096    \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 5, 64, 256)        327936    \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 5, 64, 256)        327936    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_55 (Averag (None, 5, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 5, 31, 256)        131328    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 5, 256)            1024      \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,269,889\n",
      "Trainable params: 4,269,377\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # GPU allocation\n",
    "# from gpuutils import GpuUtils\n",
    "# GpuUtils.allocate(gpu_count=1, framework='keras')\n",
    "\n",
    "# import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# for device in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "# # --------------\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "from termcolor import colored\n",
    "import time\n",
    "from toolbox import load_file, find_68_interval, models_dir\n",
    "from radiotools import helper as hp\n",
    "from PIL import Image\n",
    "\n",
    "#from scipy import stats\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, AveragePooling1D, Input, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Lambda, MaxPooling2D, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "from generator import TrainDataset, ValDataset, n_events_per_file, n_files_train, batch_size\n",
    "from constants import run_version, dataset_name, datapath, data_filename, label_filename, plots_dir, project_name, n_files, n_files_val, dataset_em, dataset_noise, test_file_ids\n",
    "# -------\n",
    "\n",
    "# Values\n",
    "feedback_freq = 1 # Only train on 1/feedback_freq of data per epoch\n",
    "architectures_dir = \"architectures\"\n",
    "learning_rate = 0.00005\n",
    "epochs = 100\n",
    "loss_function = \"mean_squared_error\"\n",
    "es_patience = 8\n",
    "es_min_delta = 0.0001 # Old value: es_min_delta = 0.0001\n",
    "# ------\n",
    "\n",
    "# Parse arguments\n",
    "# parser = argparse.ArgumentParser(description='Neural network for neutrino energy reconstruction')\n",
    "# parser.add_argument(\"run_id\", type=str ,help=\"the id of the run, eg '3.2' for run3.2\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "run_id = \"E9.TEST\"\n",
    "\n",
    "# Save the run name\n",
    "run_name = f\"run{run_id}\"\n",
    "\n",
    "# Make sure run_name is compatible with run_version\n",
    "this_run_version = run_name.split(\".\")[0]\n",
    "this_run_id = run_name.split(\".\")[1]\n",
    "assert this_run_version == run_version, f\"run_version ({run_version}) does not match the run version for this run ({this_run_version})\"\n",
    "\n",
    "# Models folder\n",
    "saved_model_dir = models_dir(run_name)\n",
    "\n",
    "# Make sure saved_models folder exists\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "# Make sure architectures folder exists\n",
    "if not os.path.exists(f\"{saved_model_dir}/{architectures_dir}\"):\n",
    "    os.makedirs(f\"{saved_model_dir}/{architectures_dir}\")\n",
    "\n",
    "\n",
    "# Model params\n",
    "conv2D_filter_size = 5\n",
    "pooling_size = 2\n",
    "amount_Conv2D_layers_per_block = 3 \n",
    "amount_Conv2D_blocks = 4\n",
    "conv2D_filter_amount = 32\n",
    "activation_function = \"relu\"\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Create model -----------\n",
    "model = Sequential()\n",
    "\n",
    "# Conv2D block 1\n",
    "model.add(Conv2D(conv2D_filter_amount, (1, conv2D_filter_size), strides=(1, 1), padding='same', activation=activation_function, input_shape=(5, 512, 1)))\n",
    "\n",
    "for _ in range(amount_Conv2D_layers_per_block-1):\n",
    "    model.add(Conv2D(conv2D_filter_amount, (1, conv2D_filter_size), strides=(1, 1), padding='same', activation=activation_function))\n",
    "\n",
    "# MaxPooling to reduce size\n",
    "model.add(AveragePooling2D(pool_size=(1, pooling_size)))\n",
    "\n",
    "for i in range(amount_Conv2D_blocks-1):\n",
    "    # Conv2D block\n",
    "    for _ in range(amount_Conv2D_layers_per_block):\n",
    "        model.add(Conv2D(conv2D_filter_amount*2**(i+1), (1, conv2D_filter_size), strides=(1, 1), padding='same', activation=activation_function))\n",
    "\n",
    "    # MaxPooling to reduce size\n",
    "    model.add(AveragePooling2D(pool_size=(1, pooling_size)))\n",
    "\n",
    "model.add(Conv2D(256, (1, 2), strides=(1, 1), activation=activation_function))\n",
    "# model.add(Conv2D(256, (5, 1), strides=(1, 1), activation=activation_function))\n",
    "\n",
    "model.add(Reshape((5, 256)))\n",
    "# model.add(Reshape((5, 256, 1)))\n",
    "\n",
    "# model.add(Conv2D(256, (5, 1), strides=(1, 1), activation=activation_function))\n",
    "\n",
    "# TimeDistributed(Conv1D)() skips the sharing of filters (kerners), or we do Conv2D\n",
    "\n",
    "# y axis filters, x axis time (eg 5, 128, nfilters), 5 is the channels in the image (RGB would be 3)\n",
    "\n",
    "# then fully conected in features, and maybe 3 in time, but thne many parameters\n",
    "\n",
    "\n",
    "# Global average pooling/global max pooling\n",
    "\n",
    "\n",
    "# Batnch normalizaiton between fully onnected layers)\n",
    "\n",
    "\n",
    "\n",
    "# Alternative 1\n",
    "# GO down to (5, 1, 256) and then do some Conv1D (or Conv2D) to (5, 256))\n",
    "# then filter 2x3 Conv2D (2x3) 64 channels then Global Pooling\n",
    "\n",
    "\n",
    "# Alternative 2 (gravitational approach)\n",
    "# (128, 128, 5)\n",
    "# t, features, antennas (after reshgape)\n",
    "# then 5x5 filter, maybe 64 of them\n",
    "# becomes  (128, 128, 64)\n",
    "\n",
    "# 1st option\n",
    "# \n",
    "\n",
    "\n",
    "# Batch normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Flatten prior to dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers (fully connected)\n",
    "model.add(Dense(1024, activation=activation_function))\n",
    "model.add(Dense(1024, activation=activation_function))\n",
    "model.add(Dense(512, activation=activation_function))\n",
    "model.add(Dense(256, activation=activation_function))\n",
    "model.add(Dense(128, activation=activation_function))\n",
    "\n",
    "# model.add(Dense(512, activation=activation_function))\n",
    "# # model.add(Dropout(.1))\n",
    "# model.add(Dense(256, activation=activation_function))\n",
    "# # model.add(Dropout(.1))\n",
    "# model.add(Dense(128, activation=activation_function))\n",
    "# # model.add(Dropout(.1))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=Adam(lr=learning_rate))\n",
    "model.summary()\n",
    "# ------------------------------------\n",
    "\n",
    "# Save the model (for opening in eg Netron)\n",
    "#model.save(f'{saved_model_dir}/{architectures_dir}/model_architecture_{run_name}.h5')\n",
    "plot_model(model, to_file=f'{saved_model_dir}/{architectures_dir}/model_architecture_{run_name}.png', show_shapes=True)\n",
    "model_json = model.to_json()\n",
    "with open(f'{saved_model_dir}/{architectures_dir}/model_architecture_{run_name}.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f00e6b041018f9c5003ba88af84c1401696fe75920157f0e0f441a09854937f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
